{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4124a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 17:32:35 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['VLLM_USE_V1'] = '1' # this should be done befre any other import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm.sampling_params import GuidedDecodingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8303f4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# standard python libraries\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Data Science librraies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "\n",
    "# Huggingface Librraies\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Logging and secrets\n",
    "import wandb\n",
    "from huggingface_hub import login, HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a349de6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python310.zip',\n",
       " '/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10',\n",
       " '/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages',\n",
       " '/tmp/tmpalea_buh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wandb & Huggingface keys\n",
    "hf_token = \"***REMOVED***\"\n",
    "wandb_api_key = \"***REMOVED***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad430e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/012/r/rx/rxh210037/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreyhaneh-rhp7\u001b[0m (\u001b[33mreyhaneh-rhp7-university-of-texas-at-dallas\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to WANDB!\n",
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Intitialize Weights & Biases\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"Successfully logged in to WANDB!\")\n",
    "else:\n",
    "    print(\"No wandb key provided. Skipping wandb login.\")\n",
    "\n",
    "if hf_token:\n",
    "    \n",
    "    # Log in to Hugging Face\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found in notebook secrets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd2faf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rxh210037/LLM/LLM_class/HW4/Part C/wandb/run-20251021_173238-pesa7zas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM/runs/pesa7zas' target=\"_blank\">PartC_Inference_vLLM</a></strong> to <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM/runs/pesa7zas' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM/runs/pesa7zas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM/runs/pesa7zas?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f98e5185ea0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"PartC_Inference_vLLM\", name=\"PartC_Inference_vLLM\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9e7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2029, 9), (10, 8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = Path(\"***/Part C/jigsaw-agile-community-rules\")\n",
    "train_Path = data_folder / \"train.csv\"\n",
    "test_Path = data_folder / \"test.csv\"\n",
    "train = pd.read_csv(train_Path)\n",
    "test = pd.read_csv(test_Path)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834d3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_prepared_path = \"./prepared_data/\"\n",
    "output_dir = \"./outputs_qwen_ruleviolation/\"\n",
    "wandb_project = \"PartC_Inference_vLLM\"\n",
    "base_model = \"Qwen/Qwen1.5-7B\"\n",
    "# hf_profile = \"your_hf_username\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62579d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = (\n",
    "    \"You are a rule compliance analyst. \"\n",
    "    \"Your task is to determine whether the following Reddit comment complies with the subreddit rule or violates it. \"\n",
    "    \"Only respond with 'complies' or 'violates'.\\n\\n\"\n",
    ")\n",
    "COMPLETE_PHRASE = \"Respond with only one word: 'complies' or 'violates'.\"\n",
    "\n",
    "# Build the full user prompt per example\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "Subreddit: r/{row['subreddit']}\n",
    "Rule: {row['rule']}\n",
    "\n",
    "Positive Examples:\n",
    "1) {row['positive_example_1']}\n",
    "{COMPLETE_PHRASE}\n",
    "\n",
    "2) {row['positive_example_2']}\n",
    "{COMPLETE_PHRASE}\n",
    "\n",
    "Negative Examples:\n",
    "1) {row['negative_example_1']}\n",
    "{COMPLETE_PHRASE}\n",
    "\n",
    "2) {row['negative_example_2']}\n",
    "{COMPLETE_PHRASE}\n",
    "\n",
    "---\n",
    "Comment: {row['body']}\n",
    "{COMPLETE_PHRASE}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c023a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inference format (no assistant replies)\n",
    "def df_to_inference_format(df):\n",
    "    return df.apply(\n",
    "        lambda r: {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": build_prompt(r)}\n",
    "            ]\n",
    "        },\n",
    "        axis=1\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f331283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 10 records to prepared_data/test_inference.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_dir = Path(\"prepared_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "test_inference_data = df_to_inference_format(test)\n",
    "# Save to JSONL\n",
    "def save_jsonl(data, filename):\n",
    "    path = output_dir / filename\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\" Saved {len(data)} records to {path}\")\n",
    "\n",
    "\n",
    "save_jsonl(test_inference_data, \"test_inference.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba261408",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"./prepared_data/test_inference.jsonl\"       # or test file path\n",
    "LORA_ADAPTER_PATH = \"./outputs_qwen_ruleviolation/checkpoint-363\"  \n",
    "BASE_MODEL  = \"Qwen/Qwen1.5-7B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76dbc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "# Sampling / decoding parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=8,\n",
    "    top_p=1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3265db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 17:32:39 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 1024, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 128, 'guided_decoding_backend': 'outlines', 'model': 'Qwen/Qwen1.5-7B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 17:32:40 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 17:32:40 [model.py:1510] Using max model len 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:32:40,170\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 17:32:40 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-21 17:32:40 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "WARNING 10-21 17:32:40 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 10-21 17:32:43 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:44 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:44 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen1.5-7B', speculative_config=None, tokenizer='Qwen/Qwen1.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen1.5-7B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:45 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m WARNING 10-21 17:32:45 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:45 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen1.5-7B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:45 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:45 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:45 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.74it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.45it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.34it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:47 [default_loader.py:267] Loading weights took 1.75 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:48 [gpu_model_runner.py:2653] Model loading took 14.9880 GiB and 2.198537 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:53 [backends.py:548] Using cache directory: /home/012/r/rx/rxh210037/.cache/vllm/torch_compile_cache/e4e93bc2fa/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:53 [backends.py:559] Dynamo bytecode transform time: 5.50 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:55 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.545 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:57 [monitor.py:34] torch.compile takes 5.50 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:58 [gpu_worker.py:298] Available KV cache memory: 11.97 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:58 [kv_cache_utils.py:1087] GPU KV cache size: 24,512 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:32:58 [kv_cache_utils.py:1091] Maximum concurrency for 1,024 tokens per request: 23.94x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:05<00:00, 11.18it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:33:08 [gpu_model_runner.py:3480] Graph capturing finished in 9 secs, took 0.96 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=516788)\u001b[0;0m INFO 10-21 17:33:08 [core.py:210] init engine (profile, create kv cache, warmup model) took 19.92 seconds\n",
      "INFO 10-21 17:33:08 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=BASE_MODEL,\n",
    "    dtype=\"bfloat16\",              # string, not torch dtype\n",
    "    trust_remote_code=True,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=128,\n",
    "    gpu_memory_utilization=0.6,\n",
    "    max_model_len=1024,             # or larger if you have VRAM\n",
    "    guided_decoding_backend=\"outlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15ce89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "# Apply the trained LoRA adapter\n",
    "lora_request = LoRARequest(\n",
    "    lora_name=\"PartC_Inference\",\n",
    "    lora_int_id=1,\n",
    "    lora_path=LORA_ADAPTER_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b33eade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_prediction(text):\n",
    "    text = text.lower().strip()\n",
    "    if \"vio\" in text:\n",
    "        return \"violates\"\n",
    "    elif \"com\" in text:\n",
    "        return \"complies\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f9dbd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 10 examples for inference\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "with open(TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        user_msg = next(m[\"content\"] for m in ex[\"messages\"] if m[\"role\"] == \"user\")\n",
    "        prompts.append(user_msg)\n",
    "\n",
    "print(f\"âœ… Loaded {len(prompts)} examples for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5c681fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1235.51it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 30.65it/s, est. speed input: 12112.37 toks/s, output: 230.97 toks/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 10 predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "predictions = []\n",
    "batch_size = 16\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating predictions\"):\n",
    "    batch = prompts[i : i + batch_size]\n",
    "    outputs = llm.generate(batch, sampling_params, lora_request=lora_request)\n",
    "    for o in outputs:\n",
    "        raw = o.outputs[0].text.strip()\n",
    "        predictions.append(normalize_prediction(raw))\n",
    "test['predicted_label'] = predictions\n",
    "print(f\"âœ… Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c49d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Prediction Summary:\n",
      "complies   : 5\n",
      "violates   : 5\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(predictions)\n",
    "print(\"\\nðŸ” Prediction Summary:\")\n",
    "for k, v in counts.items():\n",
    "    print(f\"{k:10s} : {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ec4238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1390.96it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 35.10it/s, est. speed input: 13914.10 toks/s, output: 265.32 toks/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_texts = []     # store the actual LLM outputs\n",
    "predictions = []   # normalized labels\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating predictions\"):\n",
    "    batch = prompts[i : i + batch_size]\n",
    "    outputs = llm.generate(batch, sampling_params, lora_request=lora_request)\n",
    "    for o in outputs:\n",
    "        raw = o.outputs[0].text.strip()\n",
    "        raw_texts.append(raw)\n",
    "        predictions.append(normalize_prediction(raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "930f200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2029</td>\n",
       "      <td>NEW RAP GROUP 17. CHECK US OUT https://soundcl...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hiphopheads</td>\n",
       "      <td>Hey, guys, just wanted to drop in and invite y...</td>\n",
       "      <td>Cum Swallowing Hottie Katrina Kaif Cartoon Xvi...</td>\n",
       "      <td>SD Stream Eng - [Chelsea TV USA](http://soccer...</td>\n",
       "      <td>HD Streams: |[ENG HD Stoke vs Manchester Unite...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030</td>\n",
       "      <td>Make your life comfortable. Get up to 15% Disc...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Get a lawyer and get the security camera foota...</td>\n",
       "      <td>That isn't drastic. You tried reaching out to ...</td>\n",
       "      <td>So what are you going to do with the insurance...</td>\n",
       "      <td>It's just for Austria &amp; Germany. If you still ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2031</td>\n",
       "      <td>Kickin' ass and selling underwear!\\nJust made ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>gonewild</td>\n",
       "      <td>Good story my friend. Check out my blog at  ht...</td>\n",
       "      <td>If you know what exactly you need then you don...</td>\n",
       "      <td>CENTIPEDES\\n\\nSOME BASED PATRIOTS HAVE CREATED...</td>\n",
       "      <td>[So great! Thanks for sharing.](http://www.che...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2032</td>\n",
       "      <td>watch  hooters  best  therein  http://clickan...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>Earn 50,000 bonus points with Chase Sapphire P...</td>\n",
       "      <td>Cool, front page! I made this print along with...</td>\n",
       "      <td>[Full HD Movie Online Free](http://www.flickma...</td>\n",
       "      <td>* Karambit Black Pearl\\n* 0.02137822 Float (un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2033</td>\n",
       "      <td>bitches  for free  at this point  show all  h...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>My trade link\\nhttps://steamcommunity.com/trad...</td>\n",
       "      <td>**HD** [ mio Stadium 102 HD](http://www.genti....</td>\n",
       "      <td>Infographics is an incredible method for showi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2034</td>\n",
       "      <td>Top 10 Pentakills in August https://youtu.be/Z...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>see  tits  sweet  at this point  http://sh.or...</td>\n",
       "      <td>its my asreddit https://www.youtube.com/watch?...</td>\n",
       "      <td>click here for more videos[Russian mafia messe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2035</td>\n",
       "      <td>WIN A SAMSUNG SMARTWATCH!\\n\\nhttp://www.innomd...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>[ENJOY 50,000 BONUS POINTS after $4,000 spent ...</td>\n",
       "      <td>We have high quality Canadian Polymer banknote...</td>\n",
       "      <td>i was on the same ios and try this jailbreak m...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2036</td>\n",
       "      <td>[This mixtape is lit FAM!](https://beholdthear...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>watch good one hooters there http://https://x5...</td>\n",
       "      <td>bad pussies waiting for you herein http://gour...</td>\n",
       "      <td>[Full HD Movie Online Free](http://www.flickma...</td>\n",
       "      <td>* Computers :http://livematchstreamz.blogspot....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2037</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>movies</td>\n",
       "      <td>see  there  hooters  sweet  http://url.t.aylo...</td>\n",
       "      <td>Sign up with my link and get $1,000 of free pr...</td>\n",
       "      <td>The fact that there are pro and cons to everyt...</td>\n",
       "      <td>Oh that's interesting. You should check out my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2038</td>\n",
       "      <td>Free paypal cards here!! https://www.pointspri...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>pics</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>Old and young  your girl-next-door make sex wi...</td>\n",
       "      <td>i think  the conjuring 2 is not bad movie ,The...</td>\n",
       "      <td>wow, she's so beautyful\\n&gt;[Sakura Vietnam](htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0    2029  NEW RAP GROUP 17. CHECK US OUT https://soundcl...   \n",
       "1    2030  Make your life comfortable. Get up to 15% Disc...   \n",
       "2    2031  Kickin' ass and selling underwear!\\nJust made ...   \n",
       "3    2032   watch  hooters  best  therein  http://clickan...   \n",
       "4    2033   bitches  for free  at this point  show all  h...   \n",
       "5    2034  Top 10 Pentakills in August https://youtu.be/Z...   \n",
       "6    2035  WIN A SAMSUNG SMARTWATCH!\\n\\nhttp://www.innomd...   \n",
       "7    2036  [This mixtape is lit FAM!](https://beholdthear...   \n",
       "8    2037  must be watch movie https://sites.google.com/s...   \n",
       "9    2038  Free paypal cards here!! https://www.pointspri...   \n",
       "\n",
       "                                                rule           subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...         hiphopheads   \n",
       "1  No legal advice: Do not offer or request legal...           AskReddit   \n",
       "2  No Advertising: Spam, referral links, unsolici...            gonewild   \n",
       "3  No Advertising: Spam, referral links, unsolici...     personalfinance   \n",
       "4  No Advertising: Spam, referral links, unsolici...      Showerthoughts   \n",
       "5  No Advertising: Spam, referral links, unsolici...     leagueoflegends   \n",
       "6  No Advertising: Spam, referral links, unsolici...           AskReddit   \n",
       "7  No Advertising: Spam, referral links, unsolici...  BlackPeopleTwitter   \n",
       "8  No Advertising: Spam, referral links, unsolici...              movies   \n",
       "9  No Advertising: Spam, referral links, unsolici...                pics   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  Hey, guys, just wanted to drop in and invite y...   \n",
       "1  Get a lawyer and get the security camera foota...   \n",
       "2  Good story my friend. Check out my blog at  ht...   \n",
       "3  Earn 50,000 bonus points with Chase Sapphire P...   \n",
       "4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "5  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "6  [ENJOY 50,000 BONUS POINTS after $4,000 spent ...   \n",
       "7  watch good one hooters there http://https://x5...   \n",
       "8   see  there  hooters  sweet  http://url.t.aylo...   \n",
       "9  she will come your home open her legs with  an...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  Cum Swallowing Hottie Katrina Kaif Cartoon Xvi...   \n",
       "1  That isn't drastic. You tried reaching out to ...   \n",
       "2  If you know what exactly you need then you don...   \n",
       "3  Cool, front page! I made this print along with...   \n",
       "4  My trade link\\nhttps://steamcommunity.com/trad...   \n",
       "5   see  tits  sweet  at this point  http://sh.or...   \n",
       "6  We have high quality Canadian Polymer banknote...   \n",
       "7  bad pussies waiting for you herein http://gour...   \n",
       "8  Sign up with my link and get $1,000 of free pr...   \n",
       "9  Old and young  your girl-next-door make sex wi...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  SD Stream Eng - [Chelsea TV USA](http://soccer...   \n",
       "1  So what are you going to do with the insurance...   \n",
       "2  CENTIPEDES\\n\\nSOME BASED PATRIOTS HAVE CREATED...   \n",
       "3  [Full HD Movie Online Free](http://www.flickma...   \n",
       "4  **HD** [ mio Stadium 102 HD](http://www.genti....   \n",
       "5  its my asreddit https://www.youtube.com/watch?...   \n",
       "6  i was on the same ios and try this jailbreak m...   \n",
       "7  [Full HD Movie Online Free](http://www.flickma...   \n",
       "8  The fact that there are pro and cons to everyt...   \n",
       "9  i think  the conjuring 2 is not bad movie ,The...   \n",
       "\n",
       "                                  negative_example_2  predicted_label  \n",
       "0  HD Streams: |[ENG HD Stoke vs Manchester Unite...                0  \n",
       "1  It's just for Austria & Germany. If you still ...                0  \n",
       "2  [So great! Thanks for sharing.](http://www.che...                1  \n",
       "3  * Karambit Black Pearl\\n* 0.02137822 Float (un...                1  \n",
       "4  Infographics is an incredible method for showi...                1  \n",
       "5  click here for more videos[Russian mafia messe...                0  \n",
       "6  We're streaming Pokemon Veitnamese Crystal RIG...                1  \n",
       "7  * Computers :http://livematchstreamz.blogspot....                0  \n",
       "8  Oh that's interesting. You should check out my...                0  \n",
       "9  wow, she's so beautyful\\n>[Sakura Vietnam](htt...                1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['predicted_label'] = test['predicted_label'].map({'complies': 0, 'violates': 1})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4281680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ./inference_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "#  Save predictions\n",
    "output_path = \"./inference_predictions.csv\"\n",
    "test.to_csv(output_path, index=False)\n",
    "print(f\"Saved predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a037625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PartC_Inference_vLLM</strong> at: <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM/runs/pesa7zas' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM/runs/pesa7zas</a><br> View project at: <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/PartC_Inference_vLLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_173238-pesa7zas/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
