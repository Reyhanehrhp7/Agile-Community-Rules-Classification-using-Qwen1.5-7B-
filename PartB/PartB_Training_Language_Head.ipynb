{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9885497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from huggingface_hub import login, HfApi, create_repo\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wandb & Huggingface keys\n",
    "hf_token = \"**REMOVED***\"\n",
    "wandb_api_key = \"***REMOVED***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38409926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/012/r/rx/rxh210037/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreyhaneh-rhp7\u001b[0m (\u001b[33mreyhaneh-rhp7-university-of-texas-at-dallas\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to WANDB!\n",
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Intitialize Weights & Biases\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"Successfully logged in to WANDB!\")\n",
    "else:\n",
    "    print(\"No wandb key provided. Skipping wandb login.\")\n",
    "\n",
    "if hf_token:\n",
    "    \n",
    "    # Log in to Hugging Face\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found in notebook secrets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a23166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rxh210037/LLM/LLM_class/HW4/PartB/wandb/run-20251017_172141-jiz99kil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head/runs/jiz99kil' target=\"_blank\">Jigsaw_PartB_training_Language_Head</a></strong> to <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head/runs/jiz99kil' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head/runs/jiz99kil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/Jigsaw_PartB_training_Language_Head/runs/jiz99kil?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff2c98fb2e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Jigsaw_PartB_training_Language_Head\", name=\"Jigsaw_PartB_training_Language_Head\")  # you can omit name if not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b59843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (2029, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>complies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>complies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>violates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>violates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>violates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Banks don't want you to know this! Click here ...  complies\n",
       "1  SD Stream [ ENG Link 1] (http://www.sportsstre...  complies\n",
       "2  Lol. Try appealing the ban and say you won't d...  violates\n",
       "3  she will come your home open her legs with  an...  violates\n",
       "4  code free tyrande --->>> [Imgur](http://i.imgu...  violates"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loead Data\n",
    "data_folder = Path (\"***/PartB/jigsaw-agile-community-rules\")\n",
    "train_path = data_folder / \"train.csv\"\n",
    "test_path = data_folder / \"test.csv\"\n",
    "df = pd.read_csv(train_path)\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "df.head()\n",
    "\n",
    "df[\"label\"] = df[\"rule_violation\"].map({0: \"complies\", 1: \"violates\"})\n",
    "\n",
    "stack_dataset = Dataset.from_pandas(df) # Convert pandas DataFrame to Hugging Face Dataset\n",
    "\n",
    "selected_columns = {\n",
    "    'text': stack_dataset['body'],\n",
    "    'label': stack_dataset['label']\n",
    "}\n",
    "\n",
    "# Create a new dataset with the selected columns\n",
    "stack_selected_columns = Dataset.from_dict(selected_columns)\n",
    "\n",
    "# Set the format to Pandas\n",
    "stack_selected_columns.set_format(type='pandas')\n",
    "df =stack_selected_columns[:]\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130b97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"unethical but... make a SS# but state name and address and all perfectly.  If the IRS asks, just say that's the number you got, let the IRS sort if out for you.\",\n",
       " 'label': 'violates'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"complies\", \"violates\"]\n",
    "stack_selected_columns_final = Dataset.from_pandas(df)\n",
    "\n",
    "# Split into Train / Validation / Test\n",
    "# Split the test set into test and validation sets\n",
    "test_val_splits = stack_selected_columns_final.train_test_split(test_size=0.2, seed=42)\n",
    "train_split= test_val_splits['train']\n",
    "test_val_splits = test_val_splits['test'].train_test_split(test_size=0.5, seed=42,)\n",
    "val_split = test_val_splits['train']\n",
    "test_split = test_val_splits['test']\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_split, \"valid\": val_split, \"test\": test_split})\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb3134",
   "metadata": {},
   "source": [
    "# <font color = 'indianred'>**Load pre-trained Tokenizer**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea251e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ffc70",
   "metadata": {},
   "source": [
    "#<font color = 'indianred'> **Create Completion Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0f3039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1623/1623 [00:00<00:00, 25182.86 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 22905.51 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 23822.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Classify the TEXT by selecting label from the following list: ['complies', 'violates']. ### TEXT: unethical but... make a SS# but state name and address and all perfectly.  If the IRS asks, just say that's the number you got, let the IRS sort if out for you. ### LABEL:\",\n",
       " 'completion': ' violates'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"complies\", \"violates\"]\n",
    "\n",
    "def format_prompt_completion(example):\n",
    "    prompt = f\"Classify the TEXT by selecting label from the following list: {class_names}. ### TEXT: {example['text'].strip()} ### LABEL:\"\n",
    "    completion = f\" {example['label'].strip()}\"\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "dataset_completion = dataset.map(format_prompt_completion, remove_columns=[\"text\", \"label\"])\n",
    "\n",
    "dataset_completion\n",
    "\n",
    "dataset_completion['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d7b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 154.32ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  215kB /  215kB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.17 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 625.08ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 28.3kB / 28.3kB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.40 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 556.13ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 28.6kB / 28.6kB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.40 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/reyhanehrhp7/Jigsaw_partB_language_head/commit/bd5b3e05c188bb259aa9e3f7debde07a36919c50', commit_message='Upload dataset', commit_description='', oid='bd5b3e05c188bb259aa9e3f7debde07a36919c50', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/reyhanehrhp7/Jigsaw_partB_language_head', endpoint='https://huggingface.co', repo_type='dataset', repo_id='reyhanehrhp7/Jigsaw_partB_language_head'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_completion.push_to_hub(\n",
    "    \"reyhanehrhp7/Jigsaw_partB_language_head\",\n",
    "    private=False  # Set to True if you want it private\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634391e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filltered = dataset_completion['train']\n",
    "valid_filltered = dataset_completion['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6788b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1623/1623 [00:00<00:00, 5924.17 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 4657.43 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 4258.65 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 203\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and truncate prompts to a fixed maximum length (in tokens)\n",
    "max_length = 128  # shorten sequence length further to reduce memory\n",
    "\n",
    "def tokenize_and_truncate(example):\n",
    "    tok = tokenizer(example['prompt'], truncation=True, max_length=max_length, padding=False)\n",
    "    return {'input_ids': tok['input_ids'], 'attention_mask': tok['attention_mask']}\n",
    "\n",
    "dataset_completion = dataset_completion.map(tokenize_and_truncate, remove_columns=['prompt', 'completion'], batched=False)\n",
    "dataset_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a9d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 7,729,713,152 || trainable%: 0.1085\n"
     ]
    }
   ],
   "source": [
    "def get_appropriate_dtype():\n",
    "    if torch.cuda.is_available() and torch.cuda.get_device_capability(0) >= (8, 0):\n",
    "        return torch.bfloat16\n",
    "    return torch.float16\n",
    "torch_data_type = get_appropriate_dtype()\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch_data_type,\n",
    "    bnb_4bit_quant_storage=torch_data_type,\n",
    ")\n",
    "\n",
    "## Load quantized base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Apply 4-bit quantization settings\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch_data_type,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# If a single CUDA device is available, move the whole model to that device\n",
    "if torch.cuda.is_available():\n",
    "    base_model.to('cuda')\n",
    "\n",
    "# Prepare model for k-bit training - ESSENTIAL for QLoRA\n",
    "model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# LoRA config with smaller rank for memory efficiency\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Create PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Freeze everything except LoRA layers ---\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name.lower():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Free the base model and clear cache\n",
    "del base_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2815256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "#Metrics and Training Arguments\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Ignore padding (-100)\n",
    "    mask = labels != -100\n",
    "    correct = (preds == labels) & mask\n",
    "    token_acc = correct.sum() / mask.sum()\n",
    "\n",
    "    return {\"mean_token_accuracy\": token_acc}\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results_qwen_lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    # Use fp16 for mixed precision (set bf16=False to avoid bf16-only GPUs issues)\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    # Reduce sequence length if memory is tight\n",
    "    max_length = 128,\n",
    "    # Enable gradient checkpointing to trade compute for memory\n",
    "    gradient_checkpointing=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"paged_adamw_32bit\"\n",
    ")\n",
    "\n",
    "# If gradient checkpointing is enabled, configure relevant settings\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.config.use_cache = False  # Disable caching for compatibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183f9872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|██████████| 1623/1623 [00:00<00:00, 59277.90 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1623/1623 [00:00<00:00, 3468.52 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1623/1623 [00:00<00:00, 530663.81 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 203/203 [00:00<00:00, 43074.00 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 203/203 [00:00<00:00, 3370.05 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 203/203 [00:00<00:00, 115591.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer \n",
    "# Ensure model cache is disabled for gradient checkpointing and training memory savings\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Create trainer with the PEFT model instead of base_model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Use the PEFT-wrapped model\n",
    "    args=training_args,\n",
    "    train_dataset=train_filltered,\n",
    "    eval_dataset=valid_filltered,\n",
    "    peft_config=lora_config,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238673a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=Jigsaw_PartB_training_Language_Head\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT = Jigsaw_PartB_training_Language_Head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed96b96",
   "metadata": {},
   "source": [
    "## Start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2e85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_gpu_memory():\n",
    "    \"\"\"\n",
    "    Frees up GPU memory by clearing cache and garbage collecting.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # Empty CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Get initial GPU memory info\n",
    "        initial_mem = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Get final GPU memory info\n",
    "        final_mem = torch.cuda.memory_allocated()\n",
    "        \n",
    "        print(f\"GPU memory freed: {(initial_mem - final_mem) / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c40d884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350165af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory freed: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "free_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "257a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e774192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'completion']\n"
     ]
    }
   ],
   "source": [
    "print(valid_filltered.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ed74020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: NVIDIA RTX A6000\n",
      "Memory allocated before cleaning (MiB): 5607.61083984375\n",
      "Max memory reserved (MiB): 11656.0\n",
      "GPU memory freed: 0.00 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 15:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.227301</td>\n",
       "      <td>0.883170</td>\n",
       "      <td>2.637870</td>\n",
       "      <td>118248.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.203367</td>\n",
       "      <td>0.909150</td>\n",
       "      <td>2.600972</td>\n",
       "      <td>236496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.215159</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>2.433299</td>\n",
       "      <td>354744.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/012/r/rx/rxh210037/.conda/envs/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Print GPU memory stats and clear cache before training to reduce OOM occurrences\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory allocated before cleaning (MiB):\", torch.cuda.memory_allocated() / 1024**2)\n",
    "    print(\"Max memory reserved (MiB):\", torch.cuda.max_memory_reserved() / 1024**2)\n",
    "    # Ensure cache cleared and garbage collected\n",
    "    free_gpu_memory()\n",
    "\n",
    "# Finally start training inside try/except to catch OOM and provide a graceful message\n",
    "try:\n",
    "    trainer.train()\n",
    "except RuntimeError as e:\n",
    "    if 'CUDA out of memory' in str(e):\n",
    "        print(\"CUDA out of memory error detected during trainer.train().\")\n",
    "        free_gpu_memory()\n",
    "        # Suggest next steps to the user instead of retrying automatically\n",
    "        print(\"Training OOM. Consider: lower per_device_train_batch_size, shorten max_length, increase gradient_accumulation_steps, enable/offload.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f51809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2033674120903015, 'eval_mean_token_accuracy': 0.9091503292906518, 'eval_runtime': 17.9678, 'eval_samples_per_second': 11.298, 'eval_steps_per_second': 5.677, 'eval_entropy': 2.600971795764624, 'eval_num_tokens': 354744.0, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "#evaluation on validaton set\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "582dfe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation on test set\n",
    "test_filltered = dataset_completion['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e403ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.292723178863525, 'eval_mean_token_accuracy': 0.3289932938767414, 'eval_runtime': 18.0345, 'eval_samples_per_second': 11.256, 'eval_steps_per_second': 5.656, 'eval_entropy': 2.6436598476241615, 'eval_num_tokens': 354744.0, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_filltered)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cb488fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: ./results_qwen_lora/checkpoint-52\n",
      "Best metric: 0.2033674120903015\n"
     ]
    }
   ],
   "source": [
    "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "print(\"Best metric:\", trainer.state.best_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
