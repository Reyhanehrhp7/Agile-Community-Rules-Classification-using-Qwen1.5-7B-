{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed26fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from huggingface_hub import login, HfApi, create_repo\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da21841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wandb & Huggingface keys\n",
    "hf_token = \"*****\"\n",
    "wandb_api_key = \"******\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5d1767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to WANDB!\n"
     ]
    }
   ],
   "source": [
    "# Intitialize Weights & Biases\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"Successfully logged in to WANDB!\")\n",
    "else:\n",
    "    print(\"No wandb key provided. Skipping wandb login.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb79a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "if hf_token:\n",
    "    # Log in to Hugging Face\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found in notebook secrets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f35c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (2029, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
       "2  Because this statement of his is true. It isn'...               1  \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loead Data\n",
    "data_folder = Path (\"****/jigsaw-agile-community-rules\")\n",
    "train_path = data_folder / \"train.csv\"\n",
    "test_path = data_folder / \"test.csv\"\n",
    "df = pd.read_csv(train_path)\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc8b5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"body\", \"rule_violation\"]].rename(columns={\"body\": \"text\", \"rule_violation\": \"label\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc7fedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1623 | Val: 203 | Test: 203\n"
     ]
    }
   ],
   "source": [
    "# Split into Train / Validation / Test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"label\"])\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0d8c73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_appropriate_dtype():\n",
    "    if torch.cuda.is_available() and torch.cuda.get_device_capability(0) >= (8, 0):\n",
    "        return torch.bfloat16\n",
    "    return torch.float16\n",
    "\n",
    "torch_data_type = get_appropriate_dtype()\n",
    "torch_data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4a6f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.13s/it]\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer + Model (Qwen-2-7B)\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_compute_dtype=torch_data_type,\n",
    "  bnb_4bit_quant_storage=torch_data_type,\n",
    ")\n",
    "## Load quantized base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification\n",
    "    problem_type=\"single_label_classification\",\n",
    "    quantization_config=bnb_config,  # Apply 8-bit quantization\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch_data_type,\n",
    "    trust_remote_code=True)\n",
    "  # Automatically map model to available devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3ec08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForSequenceClassification(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d86c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach a LoRA adapter (PEFT)\n",
    "\n",
    "# Prepare for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Qwen attention modules use slightly different names than LLaMA/Mistral\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24c1f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,785,408 || all params: 7,115,788,288 || trainable%: 0.2359\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7295260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def free_gpu_memory():\n",
    "#     \"\"\"\n",
    "#     Frees up GPU memory after CUDA out-of-memory error in Colab.\n",
    "\n",
    "#     This function performs the following steps:\n",
    "#     1. Deletes all PyTorch objects to clear references.\n",
    "#     2. Calls garbage collection to remove unreferenced objects from memory.\n",
    "#     3. Uses torch.cuda.empty_cache() to release cached GPU memory.\n",
    "#     4. Waits for a moment to ensure memory is fully released.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Delete all torch tensors to free up memory\n",
    "#         for obj in list(locals().values()):\n",
    "#             if torch.is_tensor(obj):\n",
    "#                 del obj\n",
    "\n",
    "#         # Collect garbage to release any remaining unused memory\n",
    "#         gc.collect()\n",
    "\n",
    "#         # Empty the CUDA cache to release GPU memory\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         # Adding a small delay to allow memory to be fully released\n",
    "#         time.sleep(2)\n",
    "\n",
    "#         print(\"GPU memory has been freed.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error while freeing GPU memory: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb613c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1623/1623 [00:00<00:00, 15145.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 12510.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 13035.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Datasets\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dfataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "val_dataset = val_dfataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2c04f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics and Training Arguments\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1  = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_qwen_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b4bb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1535576/3989333067.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rxh210037/LLM/LLM_class/HW4/wandb/run-20251013_174853-8fpuzewe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/huggingface/runs/8fpuzewe' target=\"_blank\">fast-microwave-9</a></strong> to <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/huggingface' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/huggingface/runs/8fpuzewe' target=\"_blank\">https://wandb.ai/reyhaneh-rhp7-university-of-texas-at-dallas/huggingface/runs/8fpuzewe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 27:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>22.229000</td>\n",
       "      <td>1.479141</td>\n",
       "      <td>0.615764</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.278200</td>\n",
       "      <td>0.658259</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.763819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.197200</td>\n",
       "      <td>0.666508</td>\n",
       "      <td>0.778325</td>\n",
       "      <td>0.794521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=306, training_loss=10.234817654478784, metrics={'train_runtime': 1636.6808, 'train_samples_per_second': 2.975, 'train_steps_per_second': 0.187, 'total_flos': 4.856317302590669e+16, 'train_loss': 10.234817654478784, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer = Trainer(\n",
    "    model=model,  # now a PEFT-wrapped model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31fdc497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 01:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6582591533660889,\n",
       " 'eval_accuracy': 0.7684729064039408,\n",
       " 'eval_f1': 0.7638190954773869,\n",
       " 'eval_runtime': 11.5148,\n",
       " 'eval_samples_per_second': 17.629,\n",
       " 'eval_steps_per_second': 2.258,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3b5e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7552276253700256,\n",
       " 'eval_accuracy': 0.7438423645320197,\n",
       " 'eval_f1': 0.7425742574257426,\n",
       " 'eval_runtime': 11.4942,\n",
       " 'eval_samples_per_second': 17.661,\n",
       " 'eval_steps_per_second': 2.262,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f304f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model checkpoint: ./results_qwen_lora/checkpoint-306\n"
     ]
    }
   ],
   "source": [
    "# Find the best checkpoint saved by Trainer\n",
    "import os\n",
    "\n",
    "best_model_dir = trainer.state.best_model_checkpoint\n",
    "print(\"Best model checkpoint:\", best_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4505e34",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92f0adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load that checkpoint\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = \"Qwen/Qwen1.5-7B\" \n",
    "best_lora_checkpoint = best_model_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50d4971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quantization setup (same as training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "caf93ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36063a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.16s/it]\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1ca87f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForSequenceClassification(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load LoRA adapter checkpoint\n",
    "model = PeftModel.from_pretrained(base_model, best_lora_checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb0abd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\" GPU cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddba7d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference on test set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 15.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# --- Run inference on your test data ---\n",
    "preds, probs = [], []\n",
    "\n",
    "for text in tqdm(df_test[\"body\"], desc=\"Running inference on test set\"):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "        prob = torch.softmax(logits, dim=-1)[0, 1].item()\n",
    "\n",
    "    preds.append(pred)\n",
    "    probs.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f862f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_df = pd.DataFrame({\n",
    "    \"row_id\": df_test[\"row_id\"],       # first column (from test_df)\n",
    "    \"rule_violation\": preds            # second column (model predictions)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c43a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_path = \"submission_partA_best.csv\"\n",
    "output_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
